---
title: "Dimension reduction"
output: html_notebook
---

## Het probleem

Volgens het principe van Ockhams scheermes dient een data analist altijd te streven naar een model dat het minst aantal variabelen bevat. Aan de andere kant dient het model zoveel mogelijk van de informatie uit de data terug te geven.

> “Everything must be made as simple as possible, but no simpler.” - Albert Einstein

Het vereenvoudigen van een model begint bij het reduceren van het aantal variabelen. De uitdaging is om de onafhankelijke variabelen te bewaren die de meeste informatie bevatten over de afhankelijke variabele. Hiervoor zijn verschillende methodes ontwikkeld, waarvan een aantal in deze notebook zijn verzameld.

## Principle Component Analysis (PCA) en Singular Value Decomposition (SVD)

In onderstaande video's legt Dr. Roger Peng de principes uit van PCA en SVD. De voorbeelden zijn toegevoegd in de notities onder de video's.

### Deel 1
<iframe width="560" height="315" src="https://www.youtube.com/embed/ts6UQnE6E1U" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

```{r}
set.seed(12345)
par(mar = rep(0.2, 4))
dMx <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dMx)[, nrow(dMx):1])
```

```{r}
par(mar = rep(0.2, 4))
heatmap(dMx)
```

```{r}
set.seed(678910)

for(i in 1:40) {
  cnFl <- rbinom(1, size = 1, prob = 0.5)
    if(cnFl) {
      dMx[i, ] <- dMx[i, ] + rep(c(0, 3), each = 5)
    }
}

par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dMx)[, nrow(dMx):1])
```

```{r}
par(mar = rep(0.2, 4))
heatmap(dMx)
```

```{r}
hh <- hclust(dist(dMx))
dMxOrd <- dMx[hh$order, ]
par(mfrow = c(1, 3))
image(t(dMxOrd)[, nrow(dMxOrd):1])
plot(rowMeans(dMxOrd), y = 40:1, xlab = "Row Mean", ylab = "Row", pch = 19)
plot(colMeans(dMxOrd), xlab = "Columns", ylab = "Column Mean", pch = 19)
```


### Deel 2

<iframe width="560" height="315" src="https://www.youtube.com/embed/BSfw0rpyC2g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

```{r}
svd1 <- svd(scale(dMxOrd))
par(mfrow = c(1, 3))
image(t(dMxOrd)[, nrow(dMxOrd):1])
plot(svd1$u[, 1], y = 40:1, xlab = "First left singular vector", ylab = "Row", pch = 19)
plot(svd1$v[, 1], xlab = "First right singular vector", ylab = "Column", pch = 19)
```

```{r}
par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular Value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch = 19)
```

```{r}
pca1 <- prcomp(dMxOrd, scale = TRUE)
plot(pca1$rotation[, 1], svd1$v[, 1], xlab = "Principal Component 1", ylab = "Singular Value Vector 1", pch = 19)
abline(0, 1)
```

```{r}
cstMx <- dMx*0
for(i in 1:dim(dMx)[1]) {cstMx[i, ] <- rep(c(0, 1), each = 5)}
svd1 <- svd(cstMx)
par(mfrow = c(1, 3))
image(t(cstMx)[, nrow(cstMx):1])
plot(svd1$d, xlab = "Column", ylab = "Singular Value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch = 19)
```

```{r}
set.seed(678910)

for(i in 1:40) {
  cnFl1 <- rbinom(1, size = 1, prob = 0.5)
  cnFl2 <- rbinom(1, size = 1, prob = 0.5)
    if(cnFl1) {
      dMx[i, ] <- dMx[i, ] + rep(c(0, 5), each = 5)
    }
  if(cnFl2) {
      dMx[i, ] <- dMx[i, ] + rep(c(0, 5), 5)
    }
}

hh <- hclust(dist(dMx))
dMxOrd <- dMx[hh$order, ]
```

```{r}
svd2 <- svd(scale(dMxOrd))
par(mfrow = c(1, 3))
image(t(dMxOrd)[, nrow(dMxOrd):1])
plot(rep(c(0, 1), each = 5), xlab = "Column", ylab = "Truth patter 1", pch = 19)
plot(rep(c(0, 1), 5), xlab = "Column", ylab = "Truth patter 2", pch = 19)
```

```{r}
par(mfrow = c(1, 3))
image(t(dMxOrd)[, nrow(dMxOrd):1])
plot(svd2$v[, 1], xlab = "Column", ylab = "First right singular vector", pch = 19)
plot(svd2$v[, 2], xlab = "Column", ylab = "Second right singular vector", pch = 19)
```

```{r}
par(mfrow = c(1, 2))
plot(svd2$d, xlab = "Column", ylab = "Singular Value", pch = 19)
plot(svd2$d^2/sum(svd2$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch = 19)
```

### Deel 3

<iframe width="560" height="315" src="https://www.youtube.com/embed/drNwEvEx3LY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

```{r fig.width=1.8, fig.asp=1}
load("data/face.rda")
image(t(faceData)[, nrow(faceData):1])
```

```{r}
svd3 <- svd(scale(faceData))
plot(svd3$d^2/sum(svd3$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch = 19)
```

```{r}
apx1 <- svd3$u[, 1] %*% t(svd3$v[, 1]) * svd3$d[1]
apx2 <- svd3$u[, 1:5] %*% diag(svd3$d[1:5]) %*% t(svd3$v[, 1:5])
apx3 <- svd3$u[, 1:10] %*% diag(svd3$d[1:10]) %*% t(svd3$v[, 1:10])
```

```{r fig.asp=0.25}
par(mfrow = c(1, 4))
image(t(apx1)[, nrow(apx1):1], main = "(a)")
image(t(apx2)[, nrow(apx2):1], main = "(b)")
image(t(apx3)[, nrow(apx3):1], main = "(c)")
image(t(faceData)[, nrow(faceData):1], main = "(d)")
```

## Linear Discriminant Analysis

Lees dit <a href="https://www.datascienceblog.net/post/machine-learning/linear-discriminant-analysis/" target="_blank">artikel</a> voor een uitleg. De code is overgenomen in het notebook voor verdere exploratie.

### Dat inlezen

```{r}
library(RCurl)
f <- getURL('https://www.datascienceblog.net/data-sets/phoneme.csv')
df <- read.csv(textConnection(f), header=T)
print(dim(df))
colnames(df)
```

## Splits data op in sets voor training en testen

```{r}
#logical vector: TRUE if entry belongs to train set, FALSE else
train <- grepl("^train", df$speaker)
# remove non-feature columns
to.exclude <- c("row.names", "speaker", "g")
feature.df <- df[, !colnames(df) %in% to.exclude]
test.set <- subset(feature.df, !train)
train.set <- subset(feature.df, train)
train.responses <- subset(df, train)$g
test.responses <- subset(df, !train)$g
```

### Genereer een LDA-model

```{r}
library(MASS)
lda.model <- lda(train.set, grouping = train.responses)
```

### Visualiseer de uitkomsten

```{r}
# 1. Manual transformation
# center data around weighted means & transform
means <- colSums(lda.model$prior * lda.model$means)
train.mod <- scale(train.set, center = means, scale = FALSE) %*% 
             lda.model$scaling
# 2. Use the predict function to transform:
lda.prediction.train <- predict(lda.model, train.set)
all.equal(lda.prediction.train$x, train.mod)
```

```{r}
# visualize the features in the two LDA dimensions
plot.df <- data.frame(train.mod, "Outcome" = train.responses)

library(ggplot2)
ggplot(plot.df, aes(x = LD1, y = LD2, color = Outcome)) + geom_point()
```

```{r}
library(RColorBrewer)
colors <- brewer.pal(8, "Accent")
my.cols <- colors[match(lda.prediction.train$class, levels(df$g))]
plot(lda.model, dimen = 4, col = my.cols)
```

```{r}
plot_lda_centroids <- function(lda.model, train.set, response) {
    centroids <- predict(lda.model, lda.model$means)$x
    library(RColorBrewer)
    colors <- brewer.pal(8, "Accent")
    my.cols <- colors[match(lda.prediction.train$class, levels(df$g))]
    my.points <- predict(lda.model, train.set)$x
    no.classes <- length(levels(response))
    par(mfrow=c(no.classes -1, no.classes -1), mar=c(1,1,1,1), oma=c(1,1,1,10))
    for (i in 1:(no.classes - 1)) {
        for (j in 1:(no.classes - 1)) {
            y <- my.points[, i]
            x <- my.points[, j]
            cen <- cbind(centroids[, j], centroids[, i])
            if (i == j) {
                plot(x, y, type="n") 
                max.y <- max(my.points[, i])
                max.x <- max(my.points[, j])
                min.y <- min(my.points[, i])
                min.x <- min(my.points[, j])
                max.both <- max(c(max.x, max.y))
                min.both <- max(c(min.x, min.y))
                center <- min.both + ((max.both - min.both) / 2)
                text(center, center, colnames(my.points)[i], cex = 3)}
            else {
                plot(x, y, col = my.cols, pch = as.character(response), xlab ="", ylab="")
                points(cen[,1], cen[,2], pch = 21, col = "black", bg = colors, cex = 3)
            }
        }
    }
    par(xpd = NA)
    legend(x=par("usr")[2] + 1, y = mean(par("usr")[3:4]) + 20, 
            legend = rownames(centroids), col = colors, pch = rep(20, length(colors)), cex = 3)
}
plot_lda_centroids(lda.model, train.set, train.responses)
```

```{r}
posteriors <- lda.prediction.train$posterior # N x K matrix
# MAP classification for sample 1:
pred.class <- names(which.max(posteriors[1,])) # <=> lda.prediction.train$class[1]
print(paste0("Posterior of predicted class '", pred.class, 
    "' is: ", round(posteriors[1,pred.class], 2)))

## [1] "Posterior of predicted class 'sh' is: 1"

# what are the mean posteriors for individual groups?
res <- do.call(rbind, (lapply(levels(train.responses), function(x) apply(posteriors[train.responses == x, ], 2, mean))))
rownames(res) <- levels(train.responses)
print(round(res, 3)) 
```

```{r}
dims <- 1:4 # number of canonical variables to use
accuracies <- rep(NA, length(dims))
for (i in seq_along(dims)) {
    lda.pred <- predict(lda.model, test.set, dim = dims[i])
    acc <- length(which(lda.pred$class == test.responses))/length(test.responses)
    accuracies[i] <- acc
}
reduced.df <- data.frame("Rank" = dims, "Accuracy" = round(accuracies, 2))
print(reduced.df)
```

```{r}
lda.pred <- predict(lda.model, test.set)
plot.df <- data.frame(lda.pred$x[,1:2], "Outcome" = test.responses, 
                    "Prediction" = lda.pred$class)
ggplot(plot.df, aes(x = LD1, y = LD2, color = Outcome, shape = Prediction)) +
        geom_point()
```

